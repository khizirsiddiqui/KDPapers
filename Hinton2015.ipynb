{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Knowledge Distillation",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY5TXk54rKx0",
        "colab_type": "text"
      },
      "source": [
        "# Distilling the Knowledge in a Neural Network\n",
        "\n",
        "Geoffrey Hinton, Oriol Vinyals, Jeff Dean (2015)\n",
        "\n",
        "*Paper*: [arXiv link](https://arxiv.org/pdf/1503.02531.pdf)\n",
        "\n",
        "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knA39daCq0wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljH_6x-yGpmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "mnist_image_shape = (28, 28)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               transforms.RandomCrop(mnist_image_shape, 2),\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDtDHXDwD0lw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e62c40ba-a7c6-42c8-8c9b-16f56ec0b859"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(\"Working on:\", device)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working on: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDjOLHZHB_3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TeacherNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout1=0.5, dropout2=0.5):\n",
        "        super(TeacherNetwork, self).__init__()\n",
        "        self.l1 = nn.Linear(28 * 28, 1200)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=dropout1)\n",
        "        self.l2 = nn.Linear(1200, 1200)\n",
        "        self.dropout2 = nn.Dropout(p=dropout2)\n",
        "        self.l3 = nn.Linear(1200, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.l1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.l3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXkVg7naFoVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def total_loss_accuracy(network, dataset_loader, device, lossFn=None):\n",
        "    accuracy = 0.0\n",
        "    loss = 0.0\n",
        "    dataset_size = 0\n",
        "    for j, Data in enumerate(dataset_loader, 0):\n",
        "    \tX, y = Data\n",
        "    \tX = X.to(device)\n",
        "    \ty = y.to(device)\n",
        "    \twith torch.no_grad():\n",
        "    \t\tpred = network(X)\n",
        "    \t\tif lossFn is not None:\n",
        "    \t\t\tloss += lossFn(pred, y) * y.shape[0]\n",
        "    \t\taccuracy += torch.sum(torch.argmax(pred, dim=1) == y).item()\n",
        "    \tdataset_size += y.shape[0]\n",
        "    loss = loss / dataset_size\n",
        "    accuracy = accuracy / dataset_size\n",
        "    return loss, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF3DYvMVEDCD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "d4cd7baf-eadf-4df1-b0af-a393c73d34cb"
      },
      "source": [
        "teacherModel = TeacherNetwork().to(device)\n",
        "print(teacherModel)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TeacherNetwork(\n",
            "  (l1): Linear(in_features=784, out_features=1200, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (l2): Linear(in_features=1200, out_features=1200, bias=True)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (l3): Linear(in_features=1200, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8kqx54lNbUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_main = []\n",
        "train_acc = []\n",
        "train_loss = []\n",
        "val_acc = []\n",
        "val_loss = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkOKuZgUGRue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f378c2e-773f-44ff-d49c-d2fad416024d"
      },
      "source": [
        "train_epochs = 40\n",
        "print_interval = 100\n",
        "lr = 5e-3\n",
        "\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(teacherModel.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(train_epochs):\n",
        "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "        scores = teacherModel(features.to(device))\n",
        "        loss = lossFn(scores, labels.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_main.append(loss.item())\n",
        "\n",
        "    print(\"Epoch: \", epoch, \"->\")\n",
        "    loss1, acc = total_loss_accuracy(teacherModel, train_loader, device, lossFn)\n",
        "    print(\"Accuracy Training:\", acc, end=\"\\t\")\n",
        "    loss, acc = total_loss_accuracy(teacherModel, val_loader, device, lossFn)\n",
        "    print(\"Accuracy Validation:\", acc)\n",
        "    print(\"Loss Training:\", loss1.item(), \"\\tLoss Validation:\", loss.item())\n",
        "\n",
        "print(\"Training Complete for \", train_epochs, \" epochs.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 ->\n",
            "Accuracy Training: 0.7708833333333334\tAccuracy Validation: 0.8135\n",
            "Loss Training: 0.8115366101264954 \tLoss Validation: 0.6463208794593811\n",
            "Epoch:  1 ->\n",
            "Accuracy Training: 0.7841666666666667\tAccuracy Validation: 0.8293\n",
            "Loss Training: 0.8391323089599609 \tLoss Validation: 0.6539083123207092\n",
            "Epoch:  2 ->\n",
            "Accuracy Training: 0.7935\tAccuracy Validation: 0.8319\n",
            "Loss Training: 0.7673523426055908 \tLoss Validation: 0.5969228744506836\n",
            "Epoch:  3 ->\n",
            "Accuracy Training: 0.81095\tAccuracy Validation: 0.8538\n",
            "Loss Training: 0.7228739261627197 \tLoss Validation: 0.5264754295349121\n",
            "Epoch:  4 ->\n",
            "Accuracy Training: 0.8152666666666667\tAccuracy Validation: 0.8587\n",
            "Loss Training: 0.6928107142448425 \tLoss Validation: 0.5180292725563049\n",
            "Epoch:  5 ->\n",
            "Accuracy Training: 0.8268666666666666\tAccuracy Validation: 0.8677\n",
            "Loss Training: 0.6879979968070984 \tLoss Validation: 0.5080196857452393\n",
            "Epoch:  6 ->\n",
            "Accuracy Training: 0.8128833333333333\tAccuracy Validation: 0.8571\n",
            "Loss Training: 0.7133893370628357 \tLoss Validation: 0.5550492405891418\n",
            "Epoch:  7 ->\n",
            "Accuracy Training: 0.8369333333333333\tAccuracy Validation: 0.8712\n",
            "Loss Training: 0.6520775556564331 \tLoss Validation: 0.4975806474685669\n",
            "Epoch:  8 ->\n",
            "Accuracy Training: 0.8247333333333333\tAccuracy Validation: 0.8645\n",
            "Loss Training: 0.6766408681869507 \tLoss Validation: 0.5325704216957092\n",
            "Epoch:  9 ->\n",
            "Accuracy Training: 0.8359333333333333\tAccuracy Validation: 0.8731\n",
            "Loss Training: 0.6292238235473633 \tLoss Validation: 0.47017595171928406\n",
            "Epoch:  10 ->\n",
            "Accuracy Training: 0.8377166666666667\tAccuracy Validation: 0.8779\n",
            "Loss Training: 0.6481228470802307 \tLoss Validation: 0.46967947483062744\n",
            "Epoch:  11 ->\n",
            "Accuracy Training: 0.8159\tAccuracy Validation: 0.8496\n",
            "Loss Training: 0.688663899898529 \tLoss Validation: 0.5428320169448853\n",
            "Epoch:  12 ->\n",
            "Accuracy Training: 0.8418\tAccuracy Validation: 0.885\n",
            "Loss Training: 0.6050793528556824 \tLoss Validation: 0.4704359173774719\n",
            "Epoch:  13 ->\n",
            "Accuracy Training: 0.8475\tAccuracy Validation: 0.8868\n",
            "Loss Training: 0.6109796166419983 \tLoss Validation: 0.44959139823913574\n",
            "Epoch:  14 ->\n",
            "Accuracy Training: 0.83865\tAccuracy Validation: 0.878\n",
            "Loss Training: 0.6362338066101074 \tLoss Validation: 0.4583838880062103\n",
            "Epoch:  15 ->\n",
            "Accuracy Training: 0.8456333333333333\tAccuracy Validation: 0.8817\n",
            "Loss Training: 0.6344059705734253 \tLoss Validation: 0.48576340079307556\n",
            "Epoch:  16 ->\n",
            "Accuracy Training: 0.84505\tAccuracy Validation: 0.8846\n",
            "Loss Training: 0.6064242124557495 \tLoss Validation: 0.435884028673172\n",
            "Epoch:  17 ->\n",
            "Accuracy Training: 0.8405833333333333\tAccuracy Validation: 0.8807\n",
            "Loss Training: 0.6387406587600708 \tLoss Validation: 0.46651625633239746\n",
            "Epoch:  18 ->\n",
            "Accuracy Training: 0.8416\tAccuracy Validation: 0.8893\n",
            "Loss Training: 0.6125645637512207 \tLoss Validation: 0.42096683382987976\n",
            "Epoch:  19 ->\n",
            "Accuracy Training: 0.8391833333333333\tAccuracy Validation: 0.8837\n",
            "Loss Training: 0.6523070931434631 \tLoss Validation: 0.4484759569168091\n",
            "Epoch:  20 ->\n",
            "Accuracy Training: 0.843\tAccuracy Validation: 0.885\n",
            "Loss Training: 0.6026826500892639 \tLoss Validation: 0.432634174823761\n",
            "Epoch:  21 ->\n",
            "Accuracy Training: 0.8447\tAccuracy Validation: 0.8796\n",
            "Loss Training: 0.6006439328193665 \tLoss Validation: 0.4441455006599426\n",
            "Epoch:  22 ->\n",
            "Accuracy Training: 0.8552833333333333\tAccuracy Validation: 0.8857\n",
            "Loss Training: 0.5673224925994873 \tLoss Validation: 0.441196471452713\n",
            "Epoch:  23 ->\n",
            "Accuracy Training: 0.8420333333333333\tAccuracy Validation: 0.8847\n",
            "Loss Training: 0.6163597702980042 \tLoss Validation: 0.445471853017807\n",
            "Epoch:  24 ->\n",
            "Accuracy Training: 0.8476833333333333\tAccuracy Validation: 0.8826\n",
            "Loss Training: 0.5834817290306091 \tLoss Validation: 0.44089704751968384\n",
            "Epoch:  25 ->\n",
            "Accuracy Training: 0.8519\tAccuracy Validation: 0.885\n",
            "Loss Training: 0.5806794166564941 \tLoss Validation: 0.4484032690525055\n",
            "Epoch:  26 ->\n",
            "Accuracy Training: 0.8470333333333333\tAccuracy Validation: 0.8788\n",
            "Loss Training: 0.5850270390510559 \tLoss Validation: 0.45352718234062195\n",
            "Epoch:  27 ->\n",
            "Accuracy Training: 0.84565\tAccuracy Validation: 0.8816\n",
            "Loss Training: 0.5871289968490601 \tLoss Validation: 0.4370232820510864\n",
            "Epoch:  28 ->\n",
            "Accuracy Training: 0.8468666666666667\tAccuracy Validation: 0.8807\n",
            "Loss Training: 0.6060798168182373 \tLoss Validation: 0.44646042585372925\n",
            "Epoch:  29 ->\n",
            "Accuracy Training: 0.8540333333333333\tAccuracy Validation: 0.8879\n",
            "Loss Training: 0.5697897672653198 \tLoss Validation: 0.4258151352405548\n",
            "Epoch:  30 ->\n",
            "Accuracy Training: 0.8525166666666667\tAccuracy Validation: 0.8851\n",
            "Loss Training: 0.5977017283439636 \tLoss Validation: 0.4363726079463959\n",
            "Epoch:  31 ->\n",
            "Accuracy Training: 0.8453833333333334\tAccuracy Validation: 0.8802\n",
            "Loss Training: 0.6002639532089233 \tLoss Validation: 0.46518465876579285\n",
            "Epoch:  32 ->\n",
            "Accuracy Training: 0.8497833333333333\tAccuracy Validation: 0.8849\n",
            "Loss Training: 0.5943742394447327 \tLoss Validation: 0.43458279967308044\n",
            "Epoch:  33 ->\n",
            "Accuracy Training: 0.8512333333333333\tAccuracy Validation: 0.8922\n",
            "Loss Training: 0.5735669732093811 \tLoss Validation: 0.42363691329956055\n",
            "Epoch:  34 ->\n",
            "Accuracy Training: 0.8474833333333334\tAccuracy Validation: 0.8768\n",
            "Loss Training: 0.5710029006004333 \tLoss Validation: 0.4594748020172119\n",
            "Epoch:  35 ->\n",
            "Accuracy Training: 0.85275\tAccuracy Validation: 0.8875\n",
            "Loss Training: 0.5966626405715942 \tLoss Validation: 0.4289497435092926\n",
            "Epoch:  36 ->\n",
            "Accuracy Training: 0.86235\tAccuracy Validation: 0.8894\n",
            "Loss Training: 0.582591712474823 \tLoss Validation: 0.46203017234802246\n",
            "Epoch:  37 ->\n",
            "Accuracy Training: 0.8591333333333333\tAccuracy Validation: 0.8877\n",
            "Loss Training: 0.5968710780143738 \tLoss Validation: 0.45118051767349243\n",
            "Epoch:  38 ->\n",
            "Accuracy Training: 0.8537166666666667\tAccuracy Validation: 0.8873\n",
            "Loss Training: 0.6036106944084167 \tLoss Validation: 0.4405365586280823\n",
            "Epoch:  39 ->\n",
            "Accuracy Training: 0.8536\tAccuracy Validation: 0.8878\n",
            "Loss Training: 0.5864508748054504 \tLoss Validation: 0.4271843135356903\n",
            "Training Complete for  40  epochs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABcf-FLNKDKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "698271e3-cd82-45b7-aa21-4b8004a640b5"
      },
      "source": [
        "torch.save(teacherModel.state_dict(), './teacherModel')\n",
        "!ls"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  teacherModel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvu0T2PuXVqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOAD = False\n",
        "if LOAD:\n",
        "    teacherModel = TeacherNetwork()\n",
        "    teacherModel.load_state_dict(torch.load('./teacherModel', map_location=device))  # Choose whatever GPU device number you want\n",
        "    teacherModel.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZgpmHenUQRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StudentNetwork(nn.Module):\n",
        "    def __init__(self, h1=64):\n",
        "        super(StudentNetwork, self).__init__()\n",
        "        self.l1 = nn.Linear(28 * 28, h1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.l3 = nn.Linear(h1, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.l1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.l3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEYlHBUwovlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stdModel = StudentNetwork().to(device)\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(stdModel.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9_EZkK1p1Qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax_op = nn.Softmax(dim=1)\n",
        "mseloss_fn = nn.MSELoss()\n",
        "\n",
        "def my_loss(scores, targets, temperature = 3):\n",
        "    soft_pred = softmax_op(scores / temperature)\n",
        "    soft_targets = softmax_op(targets / temperature)\n",
        "    loss = mseloss_fn(soft_pred, soft_targets)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4ZK1HS7rNX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "std_train = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHwA5oHOo-8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "87b34c3c-0851-4583-b09d-aac2874a1165"
      },
      "source": [
        "train_epochs = 5\n",
        "for epoch in range(train_epochs):\n",
        "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "        scores = stdModel(features.to(device))\n",
        "        targets = teacherModel(features.to(device))\n",
        "        loss = my_loss(scores, targets.to(device), temperature=3)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        std_train.append(loss.item())\n",
        "\n",
        "    print(\"Epoch: \", epoch, \"->\")\n",
        "    loss1, acc = total_loss_accuracy(teacherModel, train_loader, device, lossFn)\n",
        "    print(\"Accuracy Training:\", acc, end=\"\\t\")\n",
        "    loss, acc = total_loss_accuracy(teacherModel, val_loader, device, lossFn)\n",
        "    print(\"Accuracy Validation:\", acc)\n",
        "    print(\"Loss Training:\", loss1.item(), \"\\tLoss Validation:\", loss.item())\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 ->\n",
            "Accuracy Training: 0.8542166666666666\tAccuracy Validation: 0.8871\n",
            "Loss Training: 0.5685402154922485 \tLoss Validation: 0.4399680495262146\n",
            "Epoch:  1 ->\n",
            "Accuracy Training: 0.8540333333333333\tAccuracy Validation: 0.8856\n",
            "Loss Training: 0.5694134831428528 \tLoss Validation: 0.416507363319397\n",
            "Epoch:  2 ->\n",
            "Accuracy Training: 0.8537\tAccuracy Validation: 0.8923\n",
            "Loss Training: 0.5812897682189941 \tLoss Validation: 0.4282498359680176\n",
            "Epoch:  3 ->\n",
            "Accuracy Training: 0.8537666666666667\tAccuracy Validation: 0.8888\n",
            "Loss Training: 0.577438473701477 \tLoss Validation: 0.42661532759666443\n",
            "Epoch:  4 ->\n",
            "Accuracy Training: 0.8533333333333334\tAccuracy Validation: 0.889\n",
            "Loss Training: 0.5899596214294434 \tLoss Validation: 0.43650320172309875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuFc2VbnrWlz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4cf68f8-a8bc-449b-f899-3c8c76096244"
      },
      "source": [
        "std_train"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.046665437519550323,\n",
              " 0.04666648432612419,\n",
              " 0.04986218735575676,\n",
              " 0.05023883655667305,\n",
              " 0.0514102540910244,\n",
              " 0.04233106970787048,\n",
              " 0.045577917248010635,\n",
              " 0.047160785645246506,\n",
              " 0.049749959260225296,\n",
              " 0.046534694731235504,\n",
              " 0.04332394525408745,\n",
              " 0.04724808782339096,\n",
              " 0.04258120805025101,\n",
              " 0.045622196048498154,\n",
              " 0.04186750575900078,\n",
              " 0.04088243842124939,\n",
              " 0.03994714468717575,\n",
              " 0.03857836127281189,\n",
              " 0.04005708917975426,\n",
              " 0.036286093294620514,\n",
              " 0.034500736743211746,\n",
              " 0.040212664753198624,\n",
              " 0.04322151467204094,\n",
              " 0.03739115968346596,\n",
              " 0.036340679973363876,\n",
              " 0.03195996209979057,\n",
              " 0.03470965102314949,\n",
              " 0.03153304383158684,\n",
              " 0.03172901272773743,\n",
              " 0.03061499632894993,\n",
              " 0.027961110696196556,\n",
              " 0.02770097926259041,\n",
              " 0.028711099177598953,\n",
              " 0.03439578041434288,\n",
              " 0.02953920140862465,\n",
              " 0.02793644182384014,\n",
              " 0.030018797144293785,\n",
              " 0.026001622900366783,\n",
              " 0.02820567600429058,\n",
              " 0.02971387840807438,\n",
              " 0.028220875188708305,\n",
              " 0.030615806579589844,\n",
              " 0.028194312006235123,\n",
              " 0.028715873137116432,\n",
              " 0.022963151335716248,\n",
              " 0.027396071702241898,\n",
              " 0.027216101065278053,\n",
              " 0.02748321369290352,\n",
              " 0.02511359192430973,\n",
              " 0.02393457293510437,\n",
              " 0.028245998546481133,\n",
              " 0.02669910527765751,\n",
              " 0.02527785860002041,\n",
              " 0.026109999045729637,\n",
              " 0.025577256456017494,\n",
              " 0.02477039024233818,\n",
              " 0.021114779636263847,\n",
              " 0.023533953353762627,\n",
              " 0.02332267351448536,\n",
              " 0.022773031145334244,\n",
              " 0.025870531797409058,\n",
              " 0.02584344707429409,\n",
              " 0.021979687735438347,\n",
              " 0.020794864743947983,\n",
              " 0.0210382379591465,\n",
              " 0.022406747564673424,\n",
              " 0.0222152266651392,\n",
              " 0.021719330921769142,\n",
              " 0.021883675828576088,\n",
              " 0.01977703906595707,\n",
              " 0.020842859521508217,\n",
              " 0.020469341427087784,\n",
              " 0.021612757816910744,\n",
              " 0.021459123119711876,\n",
              " 0.01782393269240856,\n",
              " 0.020928362384438515,\n",
              " 0.02456766925752163,\n",
              " 0.021753370761871338,\n",
              " 0.0198658499866724,\n",
              " 0.0215714480727911,\n",
              " 0.02239261008799076,\n",
              " 0.020012501627206802,\n",
              " 0.023978959769010544,\n",
              " 0.02021377719938755,\n",
              " 0.020888015627861023,\n",
              " 0.01890997588634491,\n",
              " 0.02149178273975849,\n",
              " 0.019160956144332886,\n",
              " 0.021463995799422264,\n",
              " 0.02335941419005394,\n",
              " 0.021240249276161194,\n",
              " 0.020824631676077843,\n",
              " 0.016761062666773796,\n",
              " 0.01946147158741951,\n",
              " 0.021350035443902016,\n",
              " 0.02345403842628002,\n",
              " 0.022549500688910484,\n",
              " 0.024965649470686913,\n",
              " 0.021857842803001404,\n",
              " 0.024257123470306396,\n",
              " 0.01981574110686779,\n",
              " 0.019937535747885704,\n",
              " 0.02063898742198944,\n",
              " 0.01851886697113514,\n",
              " 0.015766171738505363,\n",
              " 0.019985981285572052,\n",
              " 0.018991557881236076,\n",
              " 0.02310623601078987,\n",
              " 0.016245752573013306,\n",
              " 0.01863199472427368,\n",
              " 0.017893046140670776,\n",
              " 0.019615253433585167,\n",
              " 0.021068057045340538,\n",
              " 0.020792338997125626,\n",
              " 0.019362514838576317,\n",
              " 0.02165203168988228,\n",
              " 0.01903022825717926,\n",
              " 0.019076744094491005,\n",
              " 0.01828288473188877,\n",
              " 0.01734580099582672,\n",
              " 0.02194090746343136,\n",
              " 0.023228904232382774,\n",
              " 0.021052183583378792,\n",
              " 0.018642608076334,\n",
              " 0.019679337739944458,\n",
              " 0.020764688029885292,\n",
              " 0.019625863060355186,\n",
              " 0.015083747915923595,\n",
              " 0.018364684656262398,\n",
              " 0.018823377788066864,\n",
              " 0.02071637474000454,\n",
              " 0.017131192609667778,\n",
              " 0.018851647153496742,\n",
              " 0.01979081891477108,\n",
              " 0.016279201954603195,\n",
              " 0.020298495888710022,\n",
              " 0.023206118494272232,\n",
              " 0.01780334673821926,\n",
              " 0.016397109255194664,\n",
              " 0.019348209723830223,\n",
              " 0.018919169902801514,\n",
              " 0.018316155299544334,\n",
              " 0.017016617581248283,\n",
              " 0.019354119896888733,\n",
              " 0.018936654552817345,\n",
              " 0.01992841064929962,\n",
              " 0.01630406081676483,\n",
              " 0.0176295917481184,\n",
              " 0.017138084396719933,\n",
              " 0.01640763320028782,\n",
              " 0.017833366990089417,\n",
              " 0.01997377537190914,\n",
              " 0.017243992537260056,\n",
              " 0.01758500747382641,\n",
              " 0.018242157995700836,\n",
              " 0.017127269878983498,\n",
              " 0.01942349597811699,\n",
              " 0.016969749704003334,\n",
              " 0.015054588206112385,\n",
              " 0.016423536464571953,\n",
              " 0.015300357714295387,\n",
              " 0.01698305271565914,\n",
              " 0.016804007813334465,\n",
              " 0.015207371674478054,\n",
              " 0.016565099358558655,\n",
              " 0.019503802061080933,\n",
              " 0.02252429537475109,\n",
              " 0.022046798840165138,\n",
              " 0.017414096742868423,\n",
              " 0.02568613551557064,\n",
              " 0.01838967204093933,\n",
              " 0.02190811187028885,\n",
              " 0.018763583153486252,\n",
              " 0.01833953522145748,\n",
              " 0.01996067725121975,\n",
              " 0.01836344599723816,\n",
              " 0.01696627400815487,\n",
              " 0.016703693196177483,\n",
              " 0.017354553565382957,\n",
              " 0.018784329295158386,\n",
              " 0.0203563179820776,\n",
              " 0.017550935968756676,\n",
              " 0.019313421100378036,\n",
              " 0.015559184364974499,\n",
              " 0.017971916124224663,\n",
              " 0.016953257843852043,\n",
              " 0.01587541215121746,\n",
              " 0.01883060298860073,\n",
              " 0.014489846304059029,\n",
              " 0.021509064361453056,\n",
              " 0.018523545935750008,\n",
              " 0.01632288470864296,\n",
              " 0.01707894541323185,\n",
              " 0.01860537938773632,\n",
              " 0.018089987337589264,\n",
              " 0.016695909202098846,\n",
              " 0.015871956944465637,\n",
              " 0.017987176775932312,\n",
              " 0.017304761335253716,\n",
              " 0.016908396035432816,\n",
              " 0.016244877129793167,\n",
              " 0.020065149292349815,\n",
              " 0.01701137237250805,\n",
              " 0.01867673173546791,\n",
              " 0.013722163625061512,\n",
              " 0.019496703520417213,\n",
              " 0.01873752288520336,\n",
              " 0.015515218488872051,\n",
              " 0.020397091284394264,\n",
              " 0.017654767259955406,\n",
              " 0.0179899875074625,\n",
              " 0.01639959029853344,\n",
              " 0.017261777073144913,\n",
              " 0.017799554392695427,\n",
              " 0.018911955878138542,\n",
              " 0.01624627225100994,\n",
              " 0.020461982116103172,\n",
              " 0.01869829185307026,\n",
              " 0.019534653052687645,\n",
              " 0.015714017674326897,\n",
              " 0.016890892758965492,\n",
              " 0.015189215540885925,\n",
              " 0.014793495647609234,\n",
              " 0.01816386729478836,\n",
              " 0.015802927315235138,\n",
              " 0.018461698666214943,\n",
              " 0.016808683052659035,\n",
              " 0.01852533221244812,\n",
              " 0.015624070540070534,\n",
              " 0.016369959339499474,\n",
              " 0.015567140653729439,\n",
              " 0.014983166940510273,\n",
              " 0.017829885706305504,\n",
              " 0.015437665395438671,\n",
              " 0.018379736691713333,\n",
              " 0.014587245881557465,\n",
              " 0.01800023578107357,\n",
              " 0.015555398538708687,\n",
              " 0.017401516437530518,\n",
              " 0.01629268191754818,\n",
              " 0.017310217022895813,\n",
              " 0.017179645597934723,\n",
              " 0.01758836768567562,\n",
              " 0.01870080642402172,\n",
              " 0.014004788361489773,\n",
              " 0.019693756476044655,\n",
              " 0.012957400642335415,\n",
              " 0.01632319949567318,\n",
              " 0.01777481846511364,\n",
              " 0.019518127664923668,\n",
              " 0.015209394507110119,\n",
              " 0.014486603438854218,\n",
              " 0.015004160813987255,\n",
              " 0.017410004511475563,\n",
              " 0.014671767130494118,\n",
              " 0.017652852460741997,\n",
              " 0.014252928085625172,\n",
              " 0.013793018646538258,\n",
              " 0.017379585653543472,\n",
              " 0.013591334223747253,\n",
              " 0.016898998990654945,\n",
              " 0.016439368948340416,\n",
              " 0.014970632269978523,\n",
              " 0.01687898300588131,\n",
              " 0.014993163757026196,\n",
              " 0.01674756221473217,\n",
              " 0.011441702954471111,\n",
              " 0.017196698114275932,\n",
              " 0.013780134730041027,\n",
              " 0.01525619626045227,\n",
              " 0.01819925382733345,\n",
              " 0.018937721848487854,\n",
              " 0.015109066851437092,\n",
              " 0.014092731289565563,\n",
              " 0.014859812334179878,\n",
              " 0.014019908383488655,\n",
              " 0.0166495181620121,\n",
              " 0.016974227502942085,\n",
              " 0.013750833459198475,\n",
              " 0.015203780494630337,\n",
              " 0.014536544680595398,\n",
              " 0.01335140597075224,\n",
              " 0.018634412437677383,\n",
              " 0.015474787913262844,\n",
              " 0.018394233658909798,\n",
              " 0.016527390107512474,\n",
              " 0.017838381230831146,\n",
              " 0.01462566014379263,\n",
              " 0.01466213446110487,\n",
              " 0.017188971862196922,\n",
              " 0.017110658809542656,\n",
              " 0.014979975298047066,\n",
              " 0.018629735335707664,\n",
              " 0.013221283443272114,\n",
              " 0.01388974767178297,\n",
              " 0.016193395480513573,\n",
              " 0.01473345048725605,\n",
              " 0.013967079110443592,\n",
              " 0.016923734918236732,\n",
              " 0.01722821220755577,\n",
              " 0.014604344964027405,\n",
              " 0.01387931127101183,\n",
              " 0.011690524406731129,\n",
              " 0.015442860312759876,\n",
              " 0.015757260844111443,\n",
              " 0.01682969368994236,\n",
              " 0.013323676772415638,\n",
              " 0.01372278667986393,\n",
              " 0.01679740659892559,\n",
              " 0.013089432381093502,\n",
              " 0.013329236768186092,\n",
              " 0.018552981317043304,\n",
              " 0.01621803641319275,\n",
              " 0.016635073348879814,\n",
              " 0.013221226632595062,\n",
              " 0.012389106675982475,\n",
              " 0.014338448643684387,\n",
              " 0.014233437366783619,\n",
              " 0.020377757027745247,\n",
              " 0.014189513400197029,\n",
              " 0.014678669162094593,\n",
              " 0.015479627065360546,\n",
              " 0.02057189866900444,\n",
              " 0.015483193099498749,\n",
              " 0.015192287042737007,\n",
              " 0.013762623071670532,\n",
              " 0.015115625225007534,\n",
              " 0.015423472039401531,\n",
              " 0.01164790615439415,\n",
              " 0.015117836184799671,\n",
              " 0.013549250550568104,\n",
              " 0.012905711308121681,\n",
              " 0.014465932734310627,\n",
              " 0.011115199886262417,\n",
              " 0.012439288198947906,\n",
              " 0.018219897523522377,\n",
              " 0.017706874758005142,\n",
              " 0.015347215346992016,\n",
              " 0.01313259918242693,\n",
              " 0.013301466591656208,\n",
              " 0.016496853902935982,\n",
              " 0.014224955812096596,\n",
              " 0.014401845633983612,\n",
              " 0.015280458144843578,\n",
              " 0.016838936135172844,\n",
              " 0.013621616177260876,\n",
              " 0.015306136570870876,\n",
              " 0.015889886766672134,\n",
              " 0.01535885501652956,\n",
              " 0.016220252960920334,\n",
              " 0.015188750810921192,\n",
              " 0.017835160717368126,\n",
              " 0.0179763026535511,\n",
              " 0.01226173248142004,\n",
              " 0.014361265115439892,\n",
              " 0.013531073927879333,\n",
              " 0.015308025293052197,\n",
              " 0.013741478323936462,\n",
              " 0.014106014743447304,\n",
              " 0.01598954014480114,\n",
              " 0.011227774433791637,\n",
              " 0.014152114279568195,\n",
              " 0.013550544157624245,\n",
              " 0.014133031480014324,\n",
              " 0.015859553590416908,\n",
              " 0.01393876876682043,\n",
              " 0.01432464737445116,\n",
              " 0.013132802210748196,\n",
              " 0.013852499425411224,\n",
              " 0.013371358625590801,\n",
              " 0.015167680568993092,\n",
              " 0.013526225462555885,\n",
              " 0.01483586709946394,\n",
              " 0.01495584100484848,\n",
              " 0.015089953318238258,\n",
              " 0.012587579898536205,\n",
              " 0.014643914066255093,\n",
              " 0.015139897353947163,\n",
              " 0.01706695556640625,\n",
              " 0.013168493285775185,\n",
              " 0.014514647424221039,\n",
              " 0.017468780279159546,\n",
              " 0.016765324398875237,\n",
              " 0.01691870391368866,\n",
              " 0.012344594113528728,\n",
              " 0.014501008205115795,\n",
              " 0.014818218536674976,\n",
              " 0.014735348522663116,\n",
              " 0.015139603987336159,\n",
              " 0.015612537041306496,\n",
              " 0.015522646717727184,\n",
              " 0.01127071212977171,\n",
              " 0.01331388670951128,\n",
              " 0.014073281548917294,\n",
              " 0.012555025517940521,\n",
              " 0.014962853863835335,\n",
              " 0.015468086116015911,\n",
              " 0.012622621841728687,\n",
              " 0.014765681698918343,\n",
              " 0.013471478596329689,\n",
              " 0.016182154417037964,\n",
              " 0.012824097648262978,\n",
              " 0.013468935154378414,\n",
              " 0.013078055344522,\n",
              " 0.012486495077610016,\n",
              " 0.015514149330556393,\n",
              " 0.01271793432533741,\n",
              " 0.011803308501839638,\n",
              " 0.013355386443436146,\n",
              " 0.013931882567703724,\n",
              " 0.013676158152520657,\n",
              " 0.017257196828722954,\n",
              " 0.012252980843186378,\n",
              " 0.013788767158985138,\n",
              " 0.014142954722046852,\n",
              " 0.015199362300336361,\n",
              " 0.014413619413971901,\n",
              " 0.010154547169804573,\n",
              " 0.012027432210743427,\n",
              " 0.014816941693425179,\n",
              " 0.013207972049713135,\n",
              " 0.012759432196617126,\n",
              " 0.01488258596509695,\n",
              " 0.014986489899456501,\n",
              " 0.012712428346276283,\n",
              " 0.013201546855270863,\n",
              " 0.013223475776612759,\n",
              " 0.012025393545627594,\n",
              " 0.013524007983505726,\n",
              " 0.014540210366249084,\n",
              " 0.013835975900292397,\n",
              " 0.013696538284420967,\n",
              " 0.01200754102319479,\n",
              " 0.012828978709876537,\n",
              " 0.013472668826580048,\n",
              " 0.012267384678125381,\n",
              " 0.012118682265281677,\n",
              " 0.01476462185382843,\n",
              " 0.012338404543697834,\n",
              " 0.015409714542329311,\n",
              " 0.01236301101744175,\n",
              " 0.01785150356590748,\n",
              " 0.015268245711922646,\n",
              " 0.012771251611411572,\n",
              " 0.013546374626457691,\n",
              " 0.012125663459300995,\n",
              " 0.013726550154387951,\n",
              " 0.010782444849610329,\n",
              " 0.012131616473197937,\n",
              " 0.014189913868904114,\n",
              " 0.015108898282051086,\n",
              " 0.016054829582571983,\n",
              " 0.014925369992852211,\n",
              " 0.013014337979257107,\n",
              " 0.012898864224553108,\n",
              " 0.013532613404095173,\n",
              " 0.014679779298603535,\n",
              " 0.014215993694961071,\n",
              " 0.01464053150266409,\n",
              " 0.013177327811717987,\n",
              " 0.014250519685447216,\n",
              " 0.013082258403301239,\n",
              " 0.011233532801270485,\n",
              " 0.01753486506640911,\n",
              " 0.013934656977653503,\n",
              " 0.01087613683193922,\n",
              " 0.01166264247149229,\n",
              " 0.016203293576836586,\n",
              " 0.013537745922803879,\n",
              " 0.012676668353378773,\n",
              " 0.012000797316432,\n",
              " 0.01569298282265663,\n",
              " 0.012535414658486843,\n",
              " 0.012796925380825996,\n",
              " 0.011780725792050362,\n",
              " 0.012791849672794342,\n",
              " 0.010612602345645428,\n",
              " 0.012942244298756123,\n",
              " 0.01420400757342577,\n",
              " 0.011471927165985107,\n",
              " 0.012843041680753231,\n",
              " 0.013106077909469604,\n",
              " 0.013011221773922443,\n",
              " 0.013781646266579628,\n",
              " 0.012609425000846386,\n",
              " 0.01180881168693304,\n",
              " 0.014970967546105385,\n",
              " 0.014870906248688698,\n",
              " 0.012884887866675854,\n",
              " 0.013631782494485378,\n",
              " 0.013702248223125935,\n",
              " 0.01232330221682787,\n",
              " 0.015692614018917084,\n",
              " 0.013631390407681465,\n",
              " 0.01217507291585207,\n",
              " 0.010967441834509373,\n",
              " 0.013352221809327602,\n",
              " 0.013869605958461761,\n",
              " 0.012496712617576122,\n",
              " 0.01102859154343605,\n",
              " 0.016533708199858665,\n",
              " 0.01511442381888628,\n",
              " 0.013210824690759182,\n",
              " 0.014088000170886517,\n",
              " 0.015392184257507324,\n",
              " 0.016727862879633904,\n",
              " 0.016269152984023094,\n",
              " 0.011067487299442291,\n",
              " 0.014207243919372559,\n",
              " 0.012479591183364391,\n",
              " 0.013165418989956379,\n",
              " 0.011922044679522514,\n",
              " 0.011632527224719524,\n",
              " 0.011101702228188515,\n",
              " 0.012917633168399334,\n",
              " 0.013307747431099415,\n",
              " 0.012755006551742554,\n",
              " 0.013756907545030117,\n",
              " 0.013088676147162914,\n",
              " 0.01515654195100069,\n",
              " 0.012468977831304073,\n",
              " 0.012546864338219166,\n",
              " 0.014681244269013405,\n",
              " 0.013400929048657417,\n",
              " 0.013027416542172432,\n",
              " 0.012935263104736805,\n",
              " 0.01172672025859356,\n",
              " 0.01094431895762682,\n",
              " 0.012984332628548145,\n",
              " 0.011430918239057064,\n",
              " 0.011624651029706001,\n",
              " 0.010993133299052715,\n",
              " 0.01343946810811758,\n",
              " 0.013354894705116749,\n",
              " 0.01259834785014391,\n",
              " 0.014269587583839893,\n",
              " 0.009528269059956074,\n",
              " 0.013831028714776039,\n",
              " 0.014675644226372242,\n",
              " 0.01262887567281723,\n",
              " 0.011969668790698051,\n",
              " 0.009332842193543911,\n",
              " 0.011962476186454296,\n",
              " 0.013279154896736145,\n",
              " 0.014642593450844288,\n",
              " 0.012028155848383904,\n",
              " 0.013013392686843872,\n",
              " 0.014737608842551708,\n",
              " 0.013707173056900501,\n",
              " 0.011752872727811337,\n",
              " 0.009847923181951046,\n",
              " 0.014205793850123882,\n",
              " 0.013922542333602905,\n",
              " 0.012456024996936321,\n",
              " 0.013900533318519592,\n",
              " 0.01326914131641388,\n",
              " 0.013052918016910553,\n",
              " 0.011277980171144009,\n",
              " 0.01308471616357565,\n",
              " 0.012354565784335136,\n",
              " 0.013007349334657192,\n",
              " 0.012094545178115368,\n",
              " 0.014729916118085384,\n",
              " 0.012345673516392708,\n",
              " 0.013999350368976593,\n",
              " 0.012504114769399166,\n",
              " 0.010999772697687149,\n",
              " 0.010558103211224079,\n",
              " 0.013793650083243847,\n",
              " 0.013604141771793365,\n",
              " 0.01230911910533905,\n",
              " 0.012542054057121277,\n",
              " 0.015172630548477173,\n",
              " 0.012949970550835133,\n",
              " 0.013334308750927448,\n",
              " 0.011293548159301281,\n",
              " 0.013003411702811718,\n",
              " 0.013210752978920937,\n",
              " 0.013147714547812939,\n",
              " 0.01226444449275732,\n",
              " 0.012692445889115334,\n",
              " 0.01259589847177267,\n",
              " 0.014474622905254364,\n",
              " 0.013261789456009865,\n",
              " 0.010513733141124249,\n",
              " 0.011879353784024715,\n",
              " 0.01260426826775074,\n",
              " 0.013824445195496082,\n",
              " 0.011863039806485176,\n",
              " 0.011652673594653606,\n",
              " 0.013652424328029156,\n",
              " 0.012233738787472248,\n",
              " 0.012924638576805592,\n",
              " 0.013892263174057007,\n",
              " 0.008833746425807476,\n",
              " 0.012514586560428143,\n",
              " 0.015532607212662697,\n",
              " 0.014363906346261501,\n",
              " 0.01594986952841282,\n",
              " 0.011852036230266094,\n",
              " 0.011233297176659107,\n",
              " 0.011807153932750225,\n",
              " 0.011110857129096985,\n",
              " 0.01232447661459446,\n",
              " 0.012945251539349556,\n",
              " 0.013601884245872498,\n",
              " 0.014765343628823757,\n",
              " 0.009964349679648876,\n",
              " 0.012515023350715637,\n",
              " 0.013261489570140839,\n",
              " 0.012288840487599373,\n",
              " 0.010601627640426159,\n",
              " 0.01102371234446764,\n",
              " 0.01368523295968771,\n",
              " 0.013965316116809845,\n",
              " 0.011621169745922089,\n",
              " 0.013521254062652588,\n",
              " 0.01296147145330906,\n",
              " 0.012391473166644573,\n",
              " 0.010558576323091984,\n",
              " 0.011997532099485397,\n",
              " 0.012602478265762329,\n",
              " 0.01367011945694685,\n",
              " 0.014240912161767483,\n",
              " 0.01336573250591755,\n",
              " 0.015944326296448708,\n",
              " 0.014155079610645771,\n",
              " 0.011633919551968575,\n",
              " 0.011821405962109566,\n",
              " 0.013993426226079464,\n",
              " 0.011979437433183193,\n",
              " 0.011569752357900143,\n",
              " 0.01025959849357605,\n",
              " 0.013057795353233814,\n",
              " 0.016267916187644005,\n",
              " 0.012340092100203037,\n",
              " 0.011232948862016201,\n",
              " 0.015486950054764748,\n",
              " 0.012184595689177513,\n",
              " 0.013344578444957733,\n",
              " 0.010981685481965542,\n",
              " 0.01248579565435648,\n",
              " 0.009473110549151897,\n",
              " 0.013373357243835926,\n",
              " 0.012519488111138344,\n",
              " 0.013173817656934261,\n",
              " 0.012143564410507679,\n",
              " 0.011853393167257309,\n",
              " 0.011464327573776245,\n",
              " 0.012493190355598927,\n",
              " 0.01423164363950491,\n",
              " 0.010281438939273357,\n",
              " 0.010871860198676586,\n",
              " 0.012984069995582104,\n",
              " 0.013574977405369282,\n",
              " 0.011196962557733059,\n",
              " 0.011698274873197079,\n",
              " 0.013478487730026245,\n",
              " 0.015422842465341091,\n",
              " 0.011023011989891529,\n",
              " 0.010162929072976112,\n",
              " 0.011020035482943058,\n",
              " 0.011970131658017635,\n",
              " 0.011374139226973057,\n",
              " 0.011582469567656517,\n",
              " 0.012459937483072281,\n",
              " 0.0112290158867836,\n",
              " 0.0118599534034729,\n",
              " 0.011418327689170837,\n",
              " 0.013556606136262417,\n",
              " 0.011669347062706947,\n",
              " 0.012646439485251904,\n",
              " 0.011821719817817211,\n",
              " 0.013093763962388039,\n",
              " 0.011823976412415504,\n",
              " 0.010263181291520596,\n",
              " 0.011996269226074219,\n",
              " 0.012670372612774372,\n",
              " 0.015170926228165627,\n",
              " 0.013222912326455116,\n",
              " 0.011687074787914753,\n",
              " 0.010769380256533623,\n",
              " 0.010922344401478767,\n",
              " 0.008595572784543037,\n",
              " 0.011208032257854939,\n",
              " 0.012959027662873268,\n",
              " 0.012222846038639545,\n",
              " 0.012612520717084408,\n",
              " 0.013602986931800842,\n",
              " 0.013763374648988247,\n",
              " 0.008793698623776436,\n",
              " 0.011197897605597973,\n",
              " 0.01189480908215046,\n",
              " 0.012139203026890755,\n",
              " 0.009551708586513996,\n",
              " 0.012822714634239674,\n",
              " 0.011457487009465694,\n",
              " 0.013075465336441994,\n",
              " 0.011028037406504154,\n",
              " 0.01001625694334507,\n",
              " 0.01266160886734724,\n",
              " 0.009961970150470734,\n",
              " 0.01200354378670454,\n",
              " 0.012628942728042603,\n",
              " 0.010592757724225521,\n",
              " 0.012817613780498505,\n",
              " 0.012057929299771786,\n",
              " 0.011923988349735737,\n",
              " 0.011081828735768795,\n",
              " 0.010447576642036438,\n",
              " 0.010885536670684814,\n",
              " 0.012037107720971107,\n",
              " 0.012777859345078468,\n",
              " 0.012321682646870613,\n",
              " 0.012346579693257809,\n",
              " 0.008854043669998646,\n",
              " 0.012694205157458782,\n",
              " 0.010635088197886944,\n",
              " 0.011499513871967793,\n",
              " 0.011143731884658337,\n",
              " 0.011102503165602684,\n",
              " 0.010362743400037289,\n",
              " 0.0116269551217556,\n",
              " 0.012879808433353901,\n",
              " 0.010588334873318672,\n",
              " 0.012468922883272171,\n",
              " 0.012127000838518143,\n",
              " 0.012708774767816067,\n",
              " 0.012905331328511238,\n",
              " 0.01546297688037157,\n",
              " 0.010186448693275452,\n",
              " 0.011987850069999695,\n",
              " 0.010952616110444069,\n",
              " 0.010277586057782173,\n",
              " 0.013509250245988369,\n",
              " 0.011537573300302029,\n",
              " 0.010934087447822094,\n",
              " 0.012438299134373665,\n",
              " 0.011738021858036518,\n",
              " 0.01159015204757452,\n",
              " 0.01087655033916235,\n",
              " 0.01228475384414196,\n",
              " 0.011822471395134926,\n",
              " 0.0115381283685565,\n",
              " 0.012207571417093277,\n",
              " 0.01156192272901535,\n",
              " 0.01087650191038847,\n",
              " 0.011394395492970943,\n",
              " 0.012561963871121407,\n",
              " 0.012057450599968433,\n",
              " 0.011330506764352322,\n",
              " 0.012159841135144234,\n",
              " 0.011839300394058228,\n",
              " 0.010665195994079113,\n",
              " 0.011999267153441906,\n",
              " 0.01047656498849392,\n",
              " 0.01226701121777296,\n",
              " 0.010356052778661251,\n",
              " 0.011301075108349323,\n",
              " 0.011933719739317894,\n",
              " 0.011603089049458504,\n",
              " 0.013846123591065407,\n",
              " 0.012098485603928566,\n",
              " 0.012830262072384357,\n",
              " 0.01019888836890459,\n",
              " 0.012242845259606838,\n",
              " 0.009156166575849056,\n",
              " 0.010908328928053379,\n",
              " 0.00936292577534914,\n",
              " 0.011936242692172527,\n",
              " 0.01191704347729683,\n",
              " 0.012836487963795662,\n",
              " 0.010747791267931461,\n",
              " 0.011626536957919598,\n",
              " 0.010378851555287838,\n",
              " 0.01275593787431717,\n",
              " 0.011904023587703705,\n",
              " 0.010019314475357533,\n",
              " 0.011510086245834827,\n",
              " 0.012866325676441193,\n",
              " 0.010913334786891937,\n",
              " 0.010900395922362804,\n",
              " 0.012941109016537666,\n",
              " 0.011430948041379452,\n",
              " 0.012125594541430473,\n",
              " 0.010594924911856651,\n",
              " 0.011596557684242725,\n",
              " 0.012921345420181751,\n",
              " 0.011893476359546185,\n",
              " 0.012057102285325527,\n",
              " 0.011822236701846123,\n",
              " 0.011730070225894451,\n",
              " 0.012177012860774994,\n",
              " 0.010280459187924862,\n",
              " 0.010360971093177795,\n",
              " 0.01388069149106741,\n",
              " 0.014650009572505951,\n",
              " 0.011952536180615425,\n",
              " 0.011541194282472134,\n",
              " 0.012630139477550983,\n",
              " 0.010083461180329323,\n",
              " 0.011644112877547741,\n",
              " 0.01299795787781477,\n",
              " 0.009429759345948696,\n",
              " 0.014031213708221912,\n",
              " 0.011261184699833393,\n",
              " 0.01155130099505186,\n",
              " 0.01211859192699194,\n",
              " 0.012518084608018398,\n",
              " 0.012120961211621761,\n",
              " 0.010703816078603268,\n",
              " 0.011622133664786816,\n",
              " 0.010634158737957478,\n",
              " 0.010826105251908302,\n",
              " 0.010027522221207619,\n",
              " 0.00929521955549717,\n",
              " 0.011307571083307266,\n",
              " 0.010246455669403076,\n",
              " 0.009731126017868519,\n",
              " 0.01145730447024107,\n",
              " 0.010776038281619549,\n",
              " 0.01414464134722948,\n",
              " 0.010827688500285149,\n",
              " 0.011363773606717587,\n",
              " 0.01107694860547781,\n",
              " 0.01108204759657383,\n",
              " 0.010879245586693287,\n",
              " 0.010691518895328045,\n",
              " 0.009801538661122322,\n",
              " 0.00925217755138874,\n",
              " 0.011667217127978802,\n",
              " 0.0127726960927248,\n",
              " 0.010414451360702515,\n",
              " 0.012914130464196205,\n",
              " 0.010138245299458504,\n",
              " 0.011455113999545574,\n",
              " 0.010876177810132504,\n",
              " 0.009965630248188972,\n",
              " 0.009910504333674908,\n",
              " 0.011428759433329105,\n",
              " 0.009982476010918617,\n",
              " 0.014449509792029858,\n",
              " 0.01281842589378357,\n",
              " 0.012323412112891674,\n",
              " 0.009702421724796295,\n",
              " 0.011405163444578648,\n",
              " 0.013013752177357674,\n",
              " 0.012315523810684681,\n",
              " 0.009489755146205425,\n",
              " 0.013156652450561523,\n",
              " 0.01246683020144701,\n",
              " 0.011659621261060238,\n",
              " 0.009512079879641533,\n",
              " 0.01020006276667118,\n",
              " 0.01196166593581438,\n",
              " 0.014181102626025677,\n",
              " 0.00913370493799448,\n",
              " 0.009830421768128872,\n",
              " 0.009340668097138405,\n",
              " 0.0111164515838027,\n",
              " 0.01208595372736454,\n",
              " 0.011337073519825935,\n",
              " 0.014583015814423561,\n",
              " 0.009138721041381359,\n",
              " 0.010145510546863079,\n",
              " 0.010856796987354755,\n",
              " 0.011313130147755146,\n",
              " 0.010341573506593704,\n",
              " 0.012369050644338131,\n",
              " 0.010741542093455791,\n",
              " 0.010775439441204071,\n",
              " 0.013543687760829926,\n",
              " 0.012293259613215923,\n",
              " 0.011285023763775826,\n",
              " 0.010915122926235199,\n",
              " 0.009467716328799725,\n",
              " 0.011935126967728138,\n",
              " 0.012226239778101444,\n",
              " 0.012966379523277283,\n",
              " 0.009036262519657612,\n",
              " 0.009643602184951305,\n",
              " 0.011250428855419159,\n",
              " 0.013363972306251526,\n",
              " 0.008996154181659222,\n",
              " 0.011987986043095589,\n",
              " 0.01116432435810566,\n",
              " 0.011178734712302685,\n",
              " 0.011424562893807888,\n",
              " 0.010334284976124763,\n",
              " 0.009038659743964672,\n",
              " 0.010100111365318298,\n",
              " 0.011178572662174702,\n",
              " 0.009358829818665981,\n",
              " 0.009888403117656708,\n",
              " 0.009706935845315456,\n",
              " 0.011831158772110939,\n",
              " 0.012299786321818829,\n",
              " 0.013082844205200672,\n",
              " 0.009972368367016315,\n",
              " 0.009058818221092224,\n",
              " 0.007810428272932768,\n",
              " 0.01226968877017498,\n",
              " 0.010914752259850502,\n",
              " 0.012766668573021889,\n",
              " 0.0114200534299016,\n",
              " 0.010748607106506824,\n",
              " 0.013075090944766998,\n",
              " 0.013862763531506062,\n",
              " 0.011035489849746227,\n",
              " 0.009201406501233578,\n",
              " 0.011527493596076965,\n",
              " 0.010280107147991657,\n",
              " 0.011327315121889114,\n",
              " 0.015222834423184395,\n",
              " 0.012649199925363064,\n",
              " 0.01052475068718195,\n",
              " 0.01100274920463562,\n",
              " 0.011113177053630352,\n",
              " 0.010717875324189663,\n",
              " 0.008915630169212818,\n",
              " 0.01130898017436266,\n",
              " 0.00968658085912466,\n",
              " 0.013100000098347664,\n",
              " 0.012305349111557007,\n",
              " 0.008348013274371624,\n",
              " 0.013365931808948517,\n",
              " 0.01131296344101429,\n",
              " 0.011377885937690735,\n",
              " 0.012529564090073109,\n",
              " 0.011239297688007355,\n",
              " 0.011585209518671036,\n",
              " 0.011646806262433529,\n",
              " 0.011126026511192322,\n",
              " 0.010875945910811424,\n",
              " 0.010528454557061195,\n",
              " 0.010671812109649181,\n",
              " 0.009074119850993156,\n",
              " 0.009888616390526295,\n",
              " 0.011883406899869442,\n",
              " 0.01152003463357687,\n",
              " 0.009603670798242092,\n",
              " 0.010479656048119068,\n",
              " 0.012133821845054626,\n",
              " 0.008289377205073833,\n",
              " 0.008701972663402557,\n",
              " 0.00966248381882906,\n",
              " 0.009794249199330807,\n",
              " 0.01028158888220787,\n",
              " 0.010596474632620811,\n",
              " 0.01051296480000019,\n",
              " 0.010874281637370586,\n",
              " 0.009620345197618008,\n",
              " 0.01062196958810091,\n",
              " 0.011347267776727676,\n",
              " 0.010945849120616913,\n",
              " 0.01152033917605877,\n",
              " 0.012378600426018238,\n",
              " 0.012120483443140984,\n",
              " 0.010370388627052307,\n",
              " 0.008338309824466705,\n",
              " 0.012060442939400673,\n",
              " 0.011387735605239868,\n",
              " 0.00892606656998396,\n",
              " 0.008972709067165852,\n",
              " 0.010411352850496769,\n",
              " 0.011297176592051983,\n",
              " 0.009646295569837093,\n",
              " 0.011150907725095749,\n",
              " 0.011870011687278748,\n",
              " 0.009939951822161674,\n",
              " 0.010805460624396801,\n",
              " 0.010031833313405514,\n",
              " 0.011978034861385822,\n",
              " 0.01152870524674654,\n",
              " 0.011694833636283875,\n",
              " 0.010143374092876911,\n",
              " 0.011565516702830791,\n",
              " 0.009546314366161823,\n",
              " 0.01042183768004179,\n",
              " 0.010294810868799686,\n",
              " 0.011485518887639046,\n",
              " 0.011354396119713783,\n",
              " 0.011047648265957832,\n",
              " 0.010479466058313847,\n",
              " 0.011410894803702831,\n",
              " 0.009873106144368649,\n",
              " 0.012994395568966866,\n",
              " 0.010724206455051899,\n",
              " 0.011460727080702782,\n",
              " 0.010120918042957783,\n",
              " 0.0129153523594141,\n",
              " 0.012745767831802368,\n",
              " 0.011236490681767464,\n",
              " 0.009535339660942554,\n",
              " 0.012217159382998943,\n",
              " 0.009617961011826992,\n",
              " 0.009952577762305737,\n",
              " 0.011660106480121613,\n",
              " 0.011477245017886162,\n",
              " 0.010962738655507565,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_r5i12gxeDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}